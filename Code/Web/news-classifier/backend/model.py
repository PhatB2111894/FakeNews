# -*- coding: utf-8 -*-
# --- C√°c th∆∞ vi·ªán c·∫ßn thi·∫øt ---
import pandas as pd
import numpy as np
import tensorflow as tf
# import seaborn as sns
# import matplotlib.pyplot as plt
# from datetime import datetime
# from sklearn.metrics import ...
from transformers import RobertaTokenizer, TFRobertaForSequenceClassification
from flask_cors import CORS
from flask import Flask, request, jsonify
import re
import unicodedata
import nltk
from deep_translator import GoogleTranslator
import shap
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
import traceback
from langdetect import detect, LangDetectException
import os

# --- C√†i ƒë·∫∑t NLTK (n·∫øu c·∫ßn) ---
try: nltk.data.find('tokenizers/punkt'); nltk.data.find('corpora/stopwords')
except nltk.downloader.DownloadError:
    print("ƒêang t·∫£i NLTK data (c√≥ th·ªÉ m·∫•t v√†i ph√∫t)...")
    nltk.download('punkt', quiet=True); nltk.download('stopwords', quiet=True)
from nltk.corpus import stopwords # T·∫°m th·ªùi kh√¥ng d√πng stopwords
stop_words_en = set(stopwords.words("english"))

# --- C√°c H·∫±ng s·ªë v√† T·∫£i M√¥ h√¨nh ---
MODEL_BASE_PATH = "D:/LuanVan/FakeNews/Code/Web/news-classifier/backend/Model/"
MODEL_NAME = "RoBERTa_pretrained_20250405_171143"
TOKENIZER_NAME = "RoBERTa_token_pretrained_20250405_171143"
# 20250305_162139 √≠ot
# 20250405_171143 fakedetecnews
MODEL_PATH = os.path.join(MODEL_BASE_PATH, MODEL_NAME)
TOKENIZER_PATH = os.path.join(MODEL_BASE_PATH, TOKENIZER_NAME)

MAX_LEN_PREDICT = 512
SHAP_MAX_LEN = 256

print("--- T·∫£i M√¥ h√¨nh v√† Tokenizer ---")
print("üîÑ ƒêang t·∫£i m√¥ h√¨nh v√†o b·ªô nh·ªõ cache...")
# ... (Ph·∫ßn try...except ƒë·ªÉ load model gi·ªØ nguy√™n nh∆∞ tr∆∞·ªõc) ...
try:
    if not os.path.exists(MODEL_PATH) or not os.path.exists(TOKENIZER_PATH):
         raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y m√¥ h√¨nh ho·∫∑c tokenizer t·∫°i ƒë∆∞·ªùng d·∫´n: {MODEL_BASE_PATH}")
    MODEL = TFRobertaForSequenceClassification.from_pretrained(MODEL_PATH)
    TOKENIZER = RobertaTokenizer.from_pretrained(TOKENIZER_PATH)
    print(f"‚úÖ M√¥ h√¨nh t·ª´ '{MODEL_NAME}' v√† Tokenizer t·ª´ '{TOKENIZER_NAME}' ƒë√£ t·∫£i th√†nh c√¥ng!")
except FileNotFoundError as fnf_error:
    print(f"‚ùå L·ªói nghi√™m tr·ªçng: {fnf_error}"); MODEL = None; TOKENIZER = None
except Exception as e:
    print(f"‚ùå L·ªói kh√¥ng x√°c ƒë·ªãnh khi t·∫£i m√¥ h√¨nh: {e}"); MODEL = None; TOKENIZER = None
print("--- T·∫£i M√¥ h√¨nh v√† Tokenizer Ho√†n t·∫•t ---")


# --- C√°c H√†m H·ªó tr·ª£ ---
def preprocess_text(text):
    """Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n c∆° b·∫£n cho ti·∫øng Anh, BAO G·ªíM x√≥a stopwords."""
    if not isinstance(text, str) or not text.strip():
        print("‚ö†Ô∏è VƒÉn b·∫£n ƒë·∫ßu v√†o r·ªóng ho·∫∑c kh√¥ng h·ª£p l·ªá cho ti·ªÅn x·ª≠ l√Ω.")
        return ""
    try:
        text = text.lower() # Chuy·ªÉn th√†nh ch·ªØ th∆∞·ªùng
        text = unicodedata.normalize("NFKC", text) # Chu·∫©n h√≥a Unicode
        text = re.sub(r"https?://\S+|www\.\S+", "", text) # X√≥a URL
        text = re.sub(r"<.*?>+", "", text) # X√≥a th·∫ª HTML (n·∫øu c√≥)
        # Gi·ªØ l·∫°i ch·ªØ c√°i, s·ªë, kho·∫£ng tr·∫Øng v√† m·ªôt s·ªë d·∫•u c√¢u c∆° b·∫£n
        text = re.sub(r"[^\w\s.,!?']", "", text)
        text = re.sub(r"\s+", " ", text).strip() # Chu·∫©n h√≥a kho·∫£ng tr·∫Øng

        # !!! B·ªé COMMENT ƒê·ªÇ X√ìA STOPWORDS !!!
        words = text.split()
        # L·ªçc b·ªè c√°c t·ª´ trong danh s√°ch stop_words_en ƒë√£ ƒë·ªãnh nghƒ©a ·ªü ƒë·∫ßu file
        filtered_words = [word for word in words if word not in stop_words_en]
        # N·ªëi c√°c t·ª´ c√≤n l·∫°i th√†nh chu·ªói m·ªõi
        text = " ".join(filtered_words)

        # !!! TR·∫¢ V·ªÄ TEXT ƒê√É L·ªåC STOPWORDS !!!
        return text
    except Exception as e:
        print(f"‚ùå L·ªói trong h√†m preprocess_text: {e}")
        return "" # Tr·∫£ v·ªÅ chu·ªói r·ªóng n·∫øu c√≥ l·ªói

def translate_text(text, src_lang="auto", target_lang="en"):
    """D·ªãch vƒÉn b·∫£n s·ª≠ d·ª•ng GoogleTranslator."""
    if not text or not isinstance(text, str) or not text.strip(): return text
    max_translate_len = 1500; text_to_translate = text[:max_translate_len]
    try:
        translated = GoogleTranslator(source=src_lang, target=target_lang).translate(text_to_translate)
        return translated if translated and isinstance(translated, str) and translated.strip() else text
    except Exception as e: print(f"‚ùå L·ªói khi d·ªãch t·ª´ '{src_lang}' sang '{target_lang}': {e}"); return text

analyzer = SentimentIntensityAnalyzer()

# --- Kh·ªüi t·∫°o Flask App ---
app = Flask(__name__)
CORS(app)

# --- ƒê·ªãnh nghƒ©a API Endpoint ---
@app.route('/classify', methods=['POST'])
def classify_text():
    """API endpoint ch√≠nh ƒë·ªÉ ph√¢n lo·∫°i tin t·ª©c."""
    if MODEL is None or TOKENIZER is None: return jsonify({"error": "L·ªói Server: M√¥ h√¨nh ch∆∞a ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng!"}), 503

    try:
        data = request.get_json(); original_text = data.get('text', ''); explain_flag = data.get('explain', False)
        print(f"\n--- Y√™u c·∫ßu m·ªõi ---"); print(f"üì• ƒê√£ nh·∫≠n: Explain={explain_flag}, Text='{original_text[:100]}...'")
        if not original_text or not isinstance(original_text, str) or not original_text.strip(): return jsonify({"error": "L·ªói: VƒÉn b·∫£n ƒë·∫ßu v√†o kh√¥ng h·ª£p l·ªá ho·∫∑c b·ªã r·ªóng."}), 400
    except Exception as e: return jsonify({"error": f"L·ªói ƒë·ªãnh d·∫°ng y√™u c·∫ßu: {str(e)}"}), 400

    detected_lang = 'en'; text_to_process = original_text
    try:
        min_len_detect = 15
        if len(original_text) >= min_len_detect: detected_lang = detect(original_text)
        else: detected_lang = 'unknown_short'
        print(f"üîç Ng√¥n ng·ªØ ph√°t hi·ªán (ho·∫∑c gi·∫£ ƒë·ªãnh): '{detected_lang}'")
        if detected_lang != 'en' and detected_lang != 'unknown_short':
            print(f"üîÑ ƒêang d·ªãch t·ª´ '{detected_lang}' sang 'en'...")
            translated = translate_text(original_text, src_lang=detected_lang, target_lang='en')
            if translated != original_text and translated.strip(): text_to_process = translated; print(f"‚úÖ ƒê√£ d·ªãch sang ti·∫øng Anh: '{text_to_process[:100]}...'")
            else: print(f"‚ö†Ô∏è D·ªãch kh√¥ng th√†nh c√¥ng ho·∫∑c k·∫øt qu·∫£ r·ªóng, ti·∫øp t·ª•c x·ª≠ l√Ω b·∫±ng vƒÉn b·∫£n g·ªëc.")
    except LangDetectException: print("‚ö†Ô∏è Kh√¥ng th·ªÉ ph√°t hi·ªán ng√¥n ng·ªØ, gi·∫£ ƒë·ªãnh l√† ti·∫øng Anh."); detected_lang = 'unknown_error'
    except Exception as e: print(f"‚ùå L·ªói trong qu√° tr√¨nh x·ª≠ l√Ω ng√¥n ng·ªØ: {e}")

    print("‚öôÔ∏è Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n...")
    processed_text = preprocess_text(text_to_process)
    if not processed_text: print("‚ùå VƒÉn b·∫£n tr·ªü n√™n r·ªóng sau ti·ªÅn x·ª≠ l√Ω."); return jsonify({"error": "L·ªói: VƒÉn b·∫£n kh√¥ng h·ª£p l·ªá sau ti·ªÅn x·ª≠ l√Ω."}), 400

    print("üß† Th·ª±c hi·ªán d·ª± ƒëo√°n...")
    try:
        inputs = TOKENIZER([processed_text], padding='max_length', truncation=True, max_length=MAX_LEN_PREDICT, return_tensors='tf')
        outputs = MODEL(inputs); logits = outputs.logits[0].numpy(); probabilities = tf.nn.softmax(logits).numpy()
    except Exception as e: print(f"‚ùå L·ªói khi m√¥ h√¨nh d·ª± ƒëo√°n: {e}"); traceback.print_exc(); return jsonify({"error": f"L·ªói khi m√¥ h√¨nh d·ª± ƒëo√°n: {str(e)}"}), 500

    fake_prob_percent = float(probabilities[0] * 100); real_prob_percent = float(probabilities[1] * 100)
    print(f"üìä K·∫øt qu·∫£ d·ª± ƒëo√°n - P(Fake): {fake_prob_percent:.2f}%, P(Real): {real_prob_percent:.2f}%")

    print("üòä T√≠nh to√°n ƒëi·ªÉm c·∫£m x√∫c...")
    sentiment_score = analyzer.polarity_scores(processed_text)['compound']
    print(f"Sentiment score: {sentiment_score:.4f}")

    top_words_en = []
    if explain_flag:
        print(f"‚è≥ T√≠nh to√°n SHAP (max_length={SHAP_MAX_LEN})...")
        try:
            def shap_predict_fn(texts_for_shap):
                if isinstance(texts_for_shap, np.ndarray): texts_for_shap = texts_for_shap.tolist()
                if isinstance(texts_for_shap, str): texts_for_shap = [texts_for_shap]
                shap_inputs = TOKENIZER(texts_for_shap, padding=True, truncation=True, return_tensors="tf", max_length=SHAP_MAX_LEN)
                return MODEL(input_ids=shap_inputs['input_ids'], attention_mask=shap_inputs['attention_mask']).logits.numpy()

            explainer = shap.Explainer(shap_predict_fn, TOKENIZER); shap_values = explainer([processed_text])
            class_index_fake = 0; shap_values_for_fake = shap_values.values[0, :, class_index_fake]; tokens_or_ids = shap_values.data[0]
            positive_contribs = [(shap_values_for_fake[i], i) for i in range(len(shap_values_for_fake)) if shap_values_for_fake[i] > 0]
            positive_contribs.sort(key=lambda item: item[0], reverse=True)
            num_top_words = 10; top_indices = [index for shap_val, index in positive_contribs[:num_top_words]]
            
            processed_tokens_shap = set(); special_tokens = [ # Danh s√°ch c√°c token ƒë·∫∑c bi·ªát c·∫ßn lo·∫°i b·ªè ho√†n to√†n
                TOKENIZER.cls_token, TOKENIZER.sep_token, TOKENIZER.pad_token,
                TOKENIZER.unk_token, TOKENIZER.mask_token, '<s>', '</s>', '<pad>', ' '
            ]
            # ƒê·∫£m b·∫£o c√°c token ƒë·∫∑c bi·ªát kh√¥ng None
            special_tokens = [tok for tok in special_tokens if tok is not None]

            for idx in top_indices:
                token_data = tokens_or_ids[idx]
                token_text = "" # Kh·ªüi t·∫°o l·∫°i cho m·ªói token
                try:
                    # --- B∆∞·ªõc 1: Decode token ID ho·∫∑c l·∫•y token string ---
                    if isinstance(token_data, (int, np.integer)):
                         # Decode IDs, gi·ªØ l·∫°i k√Ω t·ª± ƒë·∫∑c bi·ªát ban ƒë·∫ßu ƒë·ªÉ ki·ªÉm tra
                         decoded_token = TOKENIZER.decode([token_data], skip_special_tokens=False, clean_up_tokenization_spaces=False)
                    elif isinstance(token_data, str):
                         decoded_token = token_data # N·∫øu SHAP data ƒë√£ l√† string
                    else:
                         continue # B·ªè qua n·∫øu kh√¥ng ph·∫£i int ho·∫∑c string

                    # --- B∆∞·ªõc 2: Ki·ªÉm tra v√† lo·∫°i b·ªè token ƒë·∫∑c bi·ªát ---
                    # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a tr∆∞·ªõc/sau do decode
                    cleaned_token = decoded_token.strip()
                    if cleaned_token in special_tokens or not cleaned_token: # N·∫øu l√† token ƒë·∫∑c bi·ªát ho·∫∑c r·ªóng th√¨ b·ªè qua
                         continue

                    # --- !!! B∆∞·ªõc 3: B·ªè k√Ω t·ª± ƒ† ·ªü ƒë·∫ßu (n·∫øu c√≥) !!! ---
                    if cleaned_token.startswith('ƒ†'):
                        token_text = cleaned_token[1:] # L·∫•y chu·ªói t·ª´ k√Ω t·ª± th·ª© 2 tr·ªü ƒëi
                    else:
                        token_text = cleaned_token # Gi·ªØ nguy√™n n·∫øu kh√¥ng c√≥ ƒ†

                    # --- B∆∞·ªõc 4: L√†m s·∫°ch kho·∫£ng tr·∫Øng l·∫ßn cu·ªëi (ph√≤ng tr∆∞·ªùng h·ª£p decode t·∫°o ra) ---
                    token_text = token_text.strip()

                except Exception as decode_err:
                    print(f"‚ö†Ô∏è L·ªói decode/clean token SHAP t·∫°i index {idx}: {decode_err}")
                    continue # B·ªè qua token n√†y n·∫øu c√≥ l·ªói

                # --- B∆∞·ªõc 5: Th√™m v√†o list n·∫øu h·ª£p l·ªá v√† ch∆∞a c√≥ ---
                # ƒê·∫£m b·∫£o token kh√¥ng r·ªóng sau khi x·ª≠ l√Ω v√† c√≥ ƒë·ªô d√†i t·ªëi thi·ªÉu (vd: > 1)
                if token_text and len(token_text) > 1 and token_text not in processed_tokens_shap:
                     top_words_en.append(token_text)
                     processed_tokens_shap.add(token_text)
                     if len(top_words_en) >= num_top_words:
                        break # ƒê·ªß 10 t·ª´ th√¨ d·ª´ng

            print(f"‚úÖ SHAP ho√†n t·∫•t. Top words/subwords (EN, ƒë√£ b·ªè ƒ†) l√†m tƒÉng ƒë·ªô gi·∫£: {top_words_en}")
        except Exception as e: print(f"‚ùå L·ªói nghi√™m tr·ªçng trong qu√° tr√¨nh t√≠nh to√°n SHAP: {e}"); traceback.print_exc()
    else: print("‚ÑπÔ∏è B·ªè qua t√≠nh to√°n SHAP theo y√™u c·∫ßu.")

    # Lu√¥n s·ª≠ d·ª•ng k·∫øt qu·∫£ ti·∫øng Anh t·ª´ SHAP
    top_words_final = top_words_en
    print("‚ÑπÔ∏è Tr·∫£ v·ªÅ top words b·∫±ng ti·∫øng Anh.")


    # 8. Chu·∫©n b·ªã v√† Tr·∫£ v·ªÅ K·∫øt qu·∫£ JSON
    try:
        response_data = {
            "original_text": original_text,
            "processed_english_text": text_to_process,
            "detected_language": detected_lang,
            "real_probability": round(real_prob_percent, 2),
            "fake_probability": round(fake_prob_percent, 2),
            "sentiment_score": round(sentiment_score, 4),
            "top_fake_words": top_words_final, # Lu√¥n l√† list ti·∫øng Anh
        }
        print(f"‚úÖ Chu·∫©n b·ªã g·ª≠i ph·∫£n h·ªìi: {response_data}")
        return jsonify(response_data)
    except Exception as e:
        print(f"‚ùå L·ªói khi t·∫°o JSON response: {e}")
        return jsonify({"error": f"L·ªói khi t·∫°o ph·∫£n h·ªìi: {str(e)}"}), 500

# --- X·ª≠ l√Ω l·ªói chung c·ªßa Flask ---
@app.errorhandler(Exception)
def handle_exception(e):
    """X·ª≠ l√Ω c√°c l·ªói kh√¥ng mong mu·ªën kh√°c."""
    error_message = traceback.format_exc(); print(f"üí• L·ªói Server kh√¥ng x√°c ƒë·ªãnh:\n{error_message}")
    response = jsonify(error="ƒê√£ x·∫£y ra l·ªói kh√¥ng mong mu·ªën tr√™n server."); response.status_code = 500
    return response

# --- Ch·∫°y App ---
if __name__ == "__main__":
    APP_PORT = 5001; print(f"--- Kh·ªüi ch·∫°y Flask App tr√™n c·ªïng {APP_PORT} ---")
    # app.run(host='0.0.0.0', port=APP_PORT, debug=True)
    app.run(host='0.0.0.0', port=APP_PORT, debug=False)